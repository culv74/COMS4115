Team names and UNIs
William Culver WRC2120
Gabi Holley    GF2501

Project Overview

This project implements a custom lexer for a simple ASCII art programming language. The lexer reads input programs and outputs a list of tokens in the format <Token Type, Token Value>.

Define the lexical grammar
Keyword Tokens: Represent commands like draw, write, and grid.
	Pattern: draw|write|grid
Identifier Tokens: Represent ASCII art identifiers such as dog, cat, tree, etc.
	Pattern: [a-zA-Z]+
Operator Tokens: Operators used to combine images, e.g., +, /, *.
	Pattern: [+/*]
Number Tokens: Represent numerical values for duplications or grid dimensions.
	Pattern: [0-9]+
Special Symbol Tokens: Represent special symbols like parentheses (, ), comma ,, and semicolon ;.
	Pattern: [(),;]

Scanning Algorithm Implementation:

Initialize Scanner:
	Start at the initial state (S0). Set the current character index to the beginning of the input program.

State Transitions:
State S0 (Initial State):
	Read the next character.
	If the character is alphabetic (a-zA-Z), transition to State S1 (Keyword Check State).
	If the character is a digit (0-9), transition to State S2 (Number State).
	If the character is one of the operators (+, /, *), transition to State S3 (Operator State).
	If the character is a special symbol ((, ), ,, ;), transition to State S4 (Special Symbol State).
	If the character is not recognized, transition to Error State and handle the lexical error.
State S1 (Keyword Check State):
	Continue reading characters until a non-alphabetic character is encountered.
	Check if the recognized string matches a keyword (draw, write, grid).
	If yes, transition to State S6 (Keyword Accept State).
	If no, transition to State S5 (Identifier State).
State S5 (Identifier State):
	Emit <Identifier, value>.
	Return to State S0.
State S6 (Keyword Accept State):
	Emit <Keyword, value> and return to State S0.
State S2 (Number State):
	Continue reading characters until a non-digit character is encountered.
	Emit <Number, value>.
	Return to State S0.
State S3 (Operator State):
	Emit <Operator, value>.
	Return to State S0.
State S4 (Special Symbol State):
	Emit <Special Symbol, value>.
	Return to State S0.
Error State:
	Emit an error message indicating the unrecognized character and its position (line and column).
	Return to State S0 or stop scanning if the error is unrecoverable.
End of Input:
	When the end of the input is reached, emit any remaining tokens and stop the scanning process.

Handling Lexical Errors
	When an invalid character or sequence is detected, output an error indicating the location and type of error.
	Example: Error: Unexpected character '@' at line 3, column 15.

Sample input programs 

Sample Program 1: Simple Drawing
draw(dog);
Expected Tokens:
<Keyword, draw>
<Special Symbol, (>
<Identifier, dog>
<Special Symbol, )>
<Special Symbol, ;>

Sample Program 2: Simple Grid
grid(2,2);
Expected Tokens:
<Keyword, grid>
<Special Symbol, (>
<Number, 2>
<Special Symbol, ,>
<Number, 2>
<Special Symbol, )>
<Special Symbol, ;>

Sample Program 3: Combination of Drawings
draw(cat) + draw(dog);
Expected Tokens:
<Keyword, draw>
<Special Symbol, (>
<Identifier, cat>
<Special Symbol, )>
<Operator, +>
<Keyword, draw>
<Special Symbol, (>
<Identifier, dog>
<Special Symbol, )>
<Special Symbol, ;>

Sample Program 4: Error Handling
draw(@cat);
Error: Unexpected character '@' at position 6
<Keyword, draw>
<Special Symbol, (>
<Identifier, cat>
<Special Symbol, )>
<Special Symbol, ;>

Sample Program 5: Duplication
write(sun) * 3;
Expected Tokens:
<Keyword, write>
<Special Symbol, (>
<Identifier, sun>
<Special Symbol, )>
<Operator, *>
<Number, 3>
<Special Symbol, ;>


Description of code

1. Importing the `sys` module
import sys

This imports the `sys` module, which allows the program to interact with the system environment. It is primarily used to print error messages to `sys.stderr` and handle command-line arguments (`sys.argv`).

2. Defining the `Lexer` Class
class Lexer:

The `Lexer` class is where all the logic for tokenizing the input source code is implemented. It contains methods for initializing the lexer, processing characters, and generating tokens.

3. Class Variable Initialization
keywords = ['draw', 'write', 'grid']
operators = '+/*'
specialSymbols = '(),;'

These are the predefined tokens:
- keywords: Reserved words in the language that the lexer should recognize.
- operators: A set of mathematical operators that can be tokens in the language.
- specialSymbols: Symbols like parentheses, commas, and semicolons that need to be recognized.

4. Initializing the Lexer
def __init__(self, source_code):
    self.source_code = source_code
    self.position = 0
    self.length = len(source_code)

The constructor initializes the lexer with the source code and sets up two key variables:
- `position`: Keeps track of the current character being processed in the input.
- `length`: Stores the total length of the source code to determine when to stop processing.

5. Getting the Next Character
python
def get_next_char(self):
    if self.position < self.length:
        char = self.source_code[self.position]
        self.position += 1
        return char
    return None

This function retrieves the next character from the input and advances the position by one. If there are no more characters to read (when `position` exceeds `length`), it returns `None`.

6. Peeking the Next Character
def peek_next_char(self):
    if self.position < self.length:
        return self.source_code[self.position]
    return None

This function is used to look at the next character without advancing the position. Itâ€™s helpful when making decisions about multi-character tokens without losing track of the current position.

7. Tokenizing the Source Code
def tokenize(self):
    tokens = []
    state = 'S0'
    buffer = ''
    min_kw_len = min(len(kw) for kw in keywords)
    max_kw_len = max(len(kw) for kw in keywords)

The `tokenize` method is responsible for breaking the source code into tokens:
- tokens: List to store the generated tokens.
- state: Tracks the current state in the finite state machine.
- buffer: Stores characters that form multi-character tokens like keywords or identifiers.
- min_kw_len and max_kw_len: Used to track keyword lengths to optimize when to check for them.

8. Looping Through the Source Code
while self.position <= self.length:
    char = self.get_next_char()

This loop processes the source code character by character until all characters are read.

9. State S0 (Initial State)
if state == 'S0':
    if char is None:
        break
    elif char.isalpha():
        buffer += char
        state = 'S1'
    elif char.isdigit():
        buffer += char
        state = 'S2'
    elif char in operators:
        state = 'S3'
    elif char in specialSymbols:
        state = 'S4'
    elif not char.isspace():
        print(f"Error: Unexpected character '{char}' at position {self.position} not parsed.", file=sys.stderr)

State S0 is the initial state. The lexer checks the character type and transitions to other states:
  - Alphabetic characters go to State S1 to potentially form a keyword or identifier.
  - Digits go to State S2 to form numbers.
  - Operators go to State S3.
  - Special symbols go to State S4.
  - Unknown characters emit an error.

10. State S1 (Building Keywords or Identifiers)
elif state == 'S1':
    if char is not None and char.isalpha():
        buffer += char
        if len(buffer) > max_kw_len:
            state = 'S5'
        elif len(buffer) >= min_kw_len and buffer in keywords:
            state = 'S6'
    else:
        state = 'S5'

State S1 handles alphabetic characters and builds potential keywords or identifiers.
   - If the buffer matches a keyword (length between min_kw_len and max_kw_len), it transitions to State S6.
   - If the buffer exceeds the keyword length or does not match, it transitions to State S5 to handle identifiers.

11. State S6 (Keyword Accept State)
elif state == 'S6':
    tokens.append(("Keyword", buffer))
    buffer = ''
    state = 'S0'
    if char is not None:
        self.position -= 1

State S6: Accepts and finalizes a keyword token. After emitting the token, it returns to State S0.

12. State S5 (Identifier Build & Accept)
elif state == 'S5':
    if char is not None and char.isalpha():
        buffer += char
    else:
        tokens.append(("Identifier", buffer))
        buffer = ''
        state = 'S0'
        if char is not None:
            self.position -= 1

State S5: Handles identifiers (variables or function names). If no more alphabetic characters follow, it finalizes the token.

13. State S2 (Building Numbers)
elif state == 'S2':
    if char is not None and char.isdigit():
        buffer += char
    else:
        tokens.append(("Number", buffer))
        buffer = ''
        state = 'S0'
        if char is not None:
            self.position -= 1

State S2: Builds number tokens by concatenating digits. When a non-digit is encountered, it emits a number token.

14. State S3 (Operators)
elif state == 'S3':
    tokens.append(("Operator", char))
    state = 'S0'

State S3: Directly emits operator tokens when encountered.

15. State S4 (Special Symbols)
elif state == 'S4':
    tokens.append(("Special Symbol", char))
    state = 'S0'

State S4: Handles special symbols like parentheses and semicolons. The lexer immediately emits these tokens.

16. Returning the List of Tokens
return tokens

After processing the entire source code, the `tokenize` function returns the list of tokens.

17. Main Function to Run the Lexer
if __name__ == "__main__":
    if len(sys.argv) == 2:
        input_file = sys.argv[1]
        try:
            with open(input_file, "r") as f:
                source_code = f.read()
        except FileNotFoundError:
            print(f"Error: File '{input_file}' not found.", file = sys.stderr)
            sys.exit(1)
    elif len(sys.argv) == 3 and sys.argv[1] == '--input':
        source_code = sys.argv[2]
    else:
        print("Usage: python3 lexer.py <input_file> or python3 lexer.py --input '<input_code>'")
        sys.exit(1)

This section handles running the lexer from the command line:
- It reads either a file or a direct input string from the command line.
- If the input file is not found, it prints an error and exits.

18. Running the Lexer and Printing Tokens
lexer = Lexer(source_code)
tokens = lexer.tokenize()
for token in tokens:
    print(f"<{token[0]}, {token[1]}>")

After initializing the `Lexer` object with the source code, it calls the `tokenize` method to generate tokens and prints each token in the format `<TokenType, TokenValue>`.

Command-line Arguments Handling:
Checks if an input file is provided (python3 lexer.py <input_file>).
If --input is specified (python3 lexer.py --input '<input_code>'), reads source code from command line arguments.
If the input is incorrect, prints usage instructions and exits.
Lexer Object:
Creates a Lexer object with the given source_code.
Calls the tokenize method to get the tokens.
Print Tokens:
Loops through the tokens list and prints each token in the format <Token Type, Token Value>.
